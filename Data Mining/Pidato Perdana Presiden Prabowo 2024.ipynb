{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FwXzsA_1G71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ARIFIALIZEVIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ARIFIALIZEVIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Load IndoBERT model and tokenizer\n",
    "model_name = \"indobenchmark/indobert-large-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Video ID and API Key\n",
    "video_id = \"UVxNclu6w-s\"  # Video ID YouTube\n",
    "apikey = \"AIzaSyDrwVcjd1hQpsivM11bq996l1zn9xj5r38\"  # API Key\n",
    "\n",
    "\n",
    "def video_comments(video_id, apikey):\n",
    "    \"\"\"\n",
    "    Fetch comments from a YouTube video.\n",
    "    Args:\n",
    "        video_id (str): YouTube video ID.\n",
    "        apikey (str): YouTube API key.\n",
    "    Returns:\n",
    "        list: A list of comments with metadata.\n",
    "    \"\"\"\n",
    "    replies = []\n",
    "    youtube = build('youtube', 'v3', developerKey=apikey)\n",
    "    video_response = youtube.commentThreads().list(\n",
    "        part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response['items']:\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            replies.append([published, user, comment, likeCount])\n",
    "\n",
    "            # Process replies\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply in item.get('replies', {}).get('comments', []):\n",
    "                    published = reply['snippet']['publishedAt']\n",
    "                    user = reply['snippet']['authorDisplayName']\n",
    "                    repl = reply['snippet']['textDisplay']\n",
    "                    likeCount = reply['snippet']['likeCount']\n",
    "                    replies.append([published, user, repl, likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                pageToken=video_response['nextPageToken'],\n",
    "                videoId=video_id\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "    return replies\n",
    "\n",
    "\n",
    "def analyze_sentiment_indo(comment):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using IndoBERT.\n",
    "    Args:\n",
    "        comment (str): Text to analyze.\n",
    "    Returns:\n",
    "        str: Sentiment label ('positif', 'negatif', or 'netral').\n",
    "    \"\"\"\n",
    "    result = sentiment_pipeline(comment)[0]\n",
    "    label = result['label']\n",
    "    if label == \"LABEL_1\":\n",
    "        return \"positif\"\n",
    "    elif label == \"LABEL_0\":\n",
    "        return \"negatif\"\n",
    "    else:\n",
    "        return \"netral\"\n",
    "\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by normalizing and removing stopwords using NLTK.\n",
    "    Args:\n",
    "        text (str): Original text.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and keep only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()\n",
    "              and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "comments = video_comments(video_id, apikey)\n",
    "\n",
    "df = pd.DataFrame(comments, columns=[\n",
    "                  'publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "df['textDisplay'] = df['textDisplay'].apply(preprocess_text_nltk)\n",
    "df['sentiment'] = df['textDisplay'].apply(analyze_sentiment_indo)\n",
    "df\n",
    "\n",
    "# Save ke CSV\n",
    "# output_path = 'dataset/youtube_comments.csv'\n",
    "# df.to_csv(output_path, index=False)\n",
    "# print(f\"Data successfully saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM0QbTXq2OPj"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "### Membuat Word Cloud ###\n",
    "text = ' '.join(df['textDisplay'])\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    max_words=100\n",
    ").generate(text)\n",
    "\n",
    "# Plot Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hilangkan sumbu\n",
    "plt.title('Word Cloud Paslon', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJv7RAlj2bAX"
   },
   "outputs": [],
   "source": [
    "### Membuat BarPlot ###\n",
    "# Hitung jumlah komentar berdasarkan sentimen\n",
    "sentiment_counts = df['sentiment'].value_counts().reset_index()\n",
    "sentiment_counts.columns = ['sentiment', 'count']\n",
    "\n",
    "sns.barplot(\n",
    "    data=sentiment_counts,\n",
    "    x='sentiment',\n",
    "    y='count',\n",
    "    hue='sentiment',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "plt.title('Sentimen Positif vs Negatif vs Netral', fontsize=16)\n",
    "plt.xlabel('Sentimen', fontsize=12)\n",
    "plt.ylabel('Total Komentar', fontsize=12)\n",
    "plt.legend([], [], frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOW0p2mgU2pr4FP+PLVzech",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "kuliah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
